{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93938004",
   "metadata": {},
   "source": [
    "# MVP: Machine Learning & Analytics\n",
    "\n",
    "**Autor**: Rodrigo Eduardo Modesto de Abreu\n",
    "\n",
    "**Data**: 27/09/2025\n",
    "\n",
    "**Matrícula**: 4052025000009\n",
    "\n",
    "**Dataset**: [Diamond Prices](https://www.kaggle.com/datasets/nancyalaswad90/diamonds-prices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157b3ce9",
   "metadata": {},
   "source": [
    "### Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [The Problem](#the-problem)\n",
    "3. [Data Preporcessing](#data-preprocessing)\n",
    "4. [Model Processing](#model-processing)\n",
    "5. [Hyperparameter Optimization](#hyperparameter-optimization)\n",
    "6. [Best Model training and execution](#best-model-training-and-execution)\n",
    "7. [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6249e1f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "The MVP is based on the Diamond Prices dataset which was also used as part of the development of the [MVP2](https://github.com/remabreu/DiamondsPrices/tree/main).\n",
    "\n",
    "At this repository, you can find:\n",
    "* [README file](https://github.com/remabreu/DiamondsPrices/blob/main/README.md) - That describes details of the dataset\n",
    "* [Notebook](https://github.com/remabreu/DiamondsPrices/blob/main/diamonds.ipynb) - The Notebook includes the whole dataset analysis and preporcessing which is also replicated in the notebook.\n",
    "\n",
    "## The Problem\n",
    "\n",
    "The Diamonds Prices dataset provides many features to support in the prediction of the target variable as a supervised regression learning. The dataset is a common and well-known regression problem in [Kaggle](https://www.kaggle.com/datasets/nancyalaswad90/diamonds-prices). The Dataset is in the latest updated version and contains 53943 records and 11 Features (one of the attributes is the index and has no relationship with the data analysis).\n",
    "\n",
    "The is only a sample of the whole dataset due to performance limitations when executing the models. Bigger samples or using the entire dataset may bring better results on models’ execution and scoring.\n",
    "\n",
    "Exploratory Data Analysis\n",
    "\n",
    "In summary, the dataseset present as the following:\n",
    "* The dataset didn't present any missing data (only )\n",
    "* Prices column was very unbalanced and skewed distribution. \n",
    "* The uncommon prices can be regarded either outliers or not depending on how there will be the use of the Diamonds, for example, as a value maximizer (i.e. Industrial use), as a collector or bridal budget. In fact, it hasn't been observed any miss-measurement or error to also regard any outlier. \n",
    "However, table and depth measurements have few significative outliers and extrapolate the \"fence\" outside Quartile 1 and 2 through IQR method.\n",
    "* carat and price produced a strong correlation in which cut, color and clarity were adjectives of such correlation by contributing into superior prices for the same carat. This behavior was more distinctly observed on smaller/lighter carats, though. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb2e1dc",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "Data preprocessing will be responsible for:\n",
    "1. Data cleaning (small amount of empty data) \n",
    "2. Apply log transformation to ```price``` column. For ```price```, the values are log-transformed, so they can have a better behavior when standardized, meaning that outliers have been reduced throughout log transformation. So, the right-skewed, wide range variation has been reduced, and the standardization has a better effect over it.\n",
    "   - Diamond prices are highly skewed — a few very large stones cost much more than average.\n",
    "      - Log transformation \n",
    "       - Reduces skew\n",
    "       - Stabilizes variance\n",
    "       - Improves model accuracy\n",
    "But final metrics must be interpreted in real prices (USD).\n",
    "1. Cleaning outliers on columns ```table``` and ```depth``` which are the only columns that outliers are more clearly considered deviations on the measurement rather than extreme points part of the dataset.\n",
    "2. Create and execute the preprocessing pipeline using `ColumnTransformer`. It creates actually two preprocessing pipelines:\n",
    "   - One for the input features (`preprocessor_X`), scaling numeric columns and one-hot encoding categorical columns.\n",
    "   - One for the target (`preprocessor_y`), scaling the diamond prices. \n",
    "   - Training and test sets are transformed separately to prevent data leakage.\n",
    "   - The target variable (price) is also log-transformed beforehand to stabilize variance and improve model performance.\n",
    "3. `ColumnTransformer` applies different transformations to different groups of columns.\n",
    "   - Uses `StandardScaler()` to standardize numeric columns so each has mean = 0 and standard deviation = 1.\n",
    "   - This ensures numeric features are on the same scale (important for many ML models).\n",
    "   - Uses `OneHotEncoder(...)` to convert categorical variables (`cut`, `color`, `clarity`) into dummy/indicator variables.\n",
    "   - `remainder='passthrough'` means that any column not part of the pipeline transformation is left unchanged\n",
    "4. The target variable (`price`) is also scaled using StandardScaler.\n",
    "   - This is useful since the model benefits from normalized target values (e.g., linear models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "2e586f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not show warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time # to measure execution time of each model \n",
    "\n",
    "# Kaggle API\n",
    "import kagglehub\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Model Selection\n",
    "from sklearn.model_selection import train_test_split # partition the dataset into train and test (holdout)\n",
    "from sklearn.model_selection import KFold # preprare the folds to cross validation \n",
    "from sklearn.model_selection import cross_val_score # execuite cross validation\n",
    "from sklearn.model_selection import RandomizedSearchCV # hyperparameter tuning with random search\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import mean_squared_error # MSE Evaluation Metric\n",
    "from sklearn.metrics import mean_absolute_error # MAE evaluiation metric\n",
    "from sklearn.metrics import root_mean_squared_error # RMSE evaluation metric\n",
    "from sklearn.metrics import r2_score # R² evaluation metric\n",
    "from sklearn.metrics import make_scorer # to create custom metrics\n",
    "\n",
    "# Algorithms\n",
    "from sklearn.linear_model import LinearRegression # Linear Regression algorithm \n",
    "from sklearn.linear_model import Ridge # Ridge Regularization algorithm\n",
    "from sklearn.linear_model import Lasso # Lasso Regularization algorithm\n",
    "from sklearn.neighbors import KNeighborsRegressor # KNN algorithm\n",
    "from sklearn.tree import DecisionTreeRegressor # Decision Tree algorithm\n",
    "from sklearn.dummy import DummyRegressor # Baseline algorithm\n",
    "from sklearn.ensemble import RandomForestRegressor # Random Forest algorithm\n",
    "from sklearn.svm import SVR # SVM algorithm\n",
    "from xgboost import XGBRegressor # XGBoost algorithm\n",
    "\n",
    "# For displaying side by side tables\n",
    "from IPython.display import display_html\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c28418d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.13)\n",
      "Path to dataset file: C:\\Users\\rodri\\.cache\\kagglehub\\datasets\\nancyalaswad90\\diamonds-prices\\versions\\4\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.50</td>\n",
       "      <td>55.00</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.80</td>\n",
       "      <td>61.00</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.90</td>\n",
       "      <td>65.00</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.40</td>\n",
       "      <td>58.00</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.30</td>\n",
       "      <td>58.00</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  carat      cut color clarity  depth  table  price    x    y  \\\n",
       "0           1   0.23    Ideal     E     SI2  61.50  55.00    326 3.95 3.98   \n",
       "1           2   0.21  Premium     E     SI1  59.80  61.00    326 3.89 3.84   \n",
       "2           3   0.23     Good     E     VS1  56.90  65.00    327 4.05 4.07   \n",
       "3           4   0.29  Premium     I     VS2  62.40  58.00    334 4.20 4.23   \n",
       "4           5   0.31     Good     J     SI2  63.30  58.00    335 4.34 4.35   \n",
       "\n",
       "     z  \n",
       "0 2.43  \n",
       "1 2.31  \n",
       "2 2.31  \n",
       "3 2.63  \n",
       "4 2.75  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = kagglehub.dataset_download(\"nancyalaswad90/diamonds-prices\")\n",
    "\n",
    "print(\"Path to dataset file:\", path)\n",
    "\n",
    "#Store the dataset into a Dataframe object\n",
    "diamonds_df = pd.read_csv(path+\"/Diamonds Prices2022.csv\")\n",
    "# Take a sample of the data for faster processing (10% of the data)\n",
    "#df_sample = diamonds_df.sample(frac=0.04, random_state=42)\n",
    "\n",
    "diamonds_df.head()\n",
    "#df_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "279420fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>53,943.00</td>\n",
       "      <td>53943</td>\n",
       "      <td>53943</td>\n",
       "      <td>53943</td>\n",
       "      <td>53,943.00</td>\n",
       "      <td>53,943.00</td>\n",
       "      <td>53,943.00</td>\n",
       "      <td>53,943.00</td>\n",
       "      <td>53,943.00</td>\n",
       "      <td>53,943.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>G</td>\n",
       "      <td>SI1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>21551</td>\n",
       "      <td>11292</td>\n",
       "      <td>13067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.80</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.75</td>\n",
       "      <td>57.46</td>\n",
       "      <td>3,932.73</td>\n",
       "      <td>5.73</td>\n",
       "      <td>5.73</td>\n",
       "      <td>3.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.47</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.43</td>\n",
       "      <td>2.23</td>\n",
       "      <td>3,989.34</td>\n",
       "      <td>1.12</td>\n",
       "      <td>1.14</td>\n",
       "      <td>0.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.00</td>\n",
       "      <td>43.00</td>\n",
       "      <td>326.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.00</td>\n",
       "      <td>56.00</td>\n",
       "      <td>950.00</td>\n",
       "      <td>4.71</td>\n",
       "      <td>4.72</td>\n",
       "      <td>2.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.70</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>61.80</td>\n",
       "      <td>57.00</td>\n",
       "      <td>2,401.00</td>\n",
       "      <td>5.70</td>\n",
       "      <td>5.71</td>\n",
       "      <td>3.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.04</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>62.50</td>\n",
       "      <td>59.00</td>\n",
       "      <td>5,324.00</td>\n",
       "      <td>6.54</td>\n",
       "      <td>6.54</td>\n",
       "      <td>4.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>79.00</td>\n",
       "      <td>95.00</td>\n",
       "      <td>18,823.00</td>\n",
       "      <td>10.74</td>\n",
       "      <td>58.90</td>\n",
       "      <td>31.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           carat    cut  color clarity     depth     table     price  \\\n",
       "count  53,943.00  53943  53943   53943 53,943.00 53,943.00 53,943.00   \n",
       "unique       NaN      5      7       8       NaN       NaN       NaN   \n",
       "top          NaN  Ideal      G     SI1       NaN       NaN       NaN   \n",
       "freq         NaN  21551  11292   13067       NaN       NaN       NaN   \n",
       "mean        0.80    NaN    NaN     NaN     61.75     57.46  3,932.73   \n",
       "std         0.47    NaN    NaN     NaN      1.43      2.23  3,989.34   \n",
       "min         0.20    NaN    NaN     NaN     43.00     43.00    326.00   \n",
       "25%         0.40    NaN    NaN     NaN     61.00     56.00    950.00   \n",
       "50%         0.70    NaN    NaN     NaN     61.80     57.00  2,401.00   \n",
       "75%         1.04    NaN    NaN     NaN     62.50     59.00  5,324.00   \n",
       "max         5.01    NaN    NaN     NaN     79.00     95.00 18,823.00   \n",
       "\n",
       "               x         y         z  \n",
       "count  53,943.00 53,943.00 53,943.00  \n",
       "unique       NaN       NaN       NaN  \n",
       "top          NaN       NaN       NaN  \n",
       "freq         NaN       NaN       NaN  \n",
       "mean        5.73      5.73      3.54  \n",
       "std         1.12      1.14      0.71  \n",
       "min         0.00      0.00      0.00  \n",
       "25%         4.71      4.72      2.91  \n",
       "50%         5.70      5.71      3.53  \n",
       "75%         6.54      6.54      4.04  \n",
       "max        10.74     58.90     31.80  "
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop first column, ignore error in case culumn doesn't exist (already removed)\n",
    "diamonds_df = diamonds_df.drop('Unnamed: 0', axis=1, errors='ignore')\n",
    "diamonds_df.describe(include='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "c795d271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with 0 or empty values:  20\n",
      "Removing rows with 0 or empty values\n",
      "Rows with 0 or empty values:  0\n"
     ]
    }
   ],
   "source": [
    "# Check for 0 or empty values in 'x', 'y', 'z' columns\n",
    "#\n",
    "print(\"Rows with 0 or empty values: \", ((diamonds_df['x'] == 0) | (diamonds_df['y'] == 0) | (diamonds_df['z'] == 0)).sum())\n",
    "print(\"Removing rows with 0 or empty values\")\n",
    "diamonds_df = diamonds_df[(diamonds_df['x'] != 0) & (diamonds_df['y'] != 0) & (diamonds_df['z'] != 0)]\n",
    "print(\"Rows with 0 or empty values: \", ((diamonds_df['x'] == 0) | (diamonds_df['y'] == 0) | (diamonds_df['z'] == 0)).sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "88244b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Separate features and target\n",
    "X = diamonds_df.drop(columns='price')\n",
    "y = diamonds_df['price']\n",
    "\n",
    "# Step 2: Apply transformation to y\n",
    "y_log = np.log1p(y)\n",
    "y_log = y_log.to_frame()\n",
    "\n",
    "# Step 3: Train/test split\n",
    "# test_size: represents the proportion of the dataset to be allocated to the test set\n",
    "# random_state: get the same split of data every time the code is executed\n",
    "X_train, X_test, y_train_log, y_test_log = train_test_split(X, y_log,\n",
    "                                                            test_size=0.2,\n",
    "                                                            random_state=SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "83f78bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iqr_filter(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "# Apply IQR filtering on training set only\n",
    "# merge sets (X) and (y) to apply filter\n",
    "train = X_train.copy()\n",
    "train['price'] = y_train_log\n",
    "train = iqr_filter(train, 'table')\n",
    "train = iqr_filter(train, 'depth')\n",
    "\n",
    "# Separate back\n",
    "y_train_log = train['price']\n",
    "X_train = train.drop(columns='price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "d12e311e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10785, 9)\n",
      "(10785, 1)\n",
      "(40497, 23)\n",
      "(10785, 23)\n",
      "(40497,)\n"
     ]
    }
   ],
   "source": [
    "# pipeline preprocessing\n",
    "\n",
    "X_num_cols = ['carat', 'table', 'depth', 'x', 'y', 'z']\n",
    "X_cat_cols = ['cut', 'color', 'clarity']\n",
    "y_num_col = ['price']\n",
    "\n",
    "# The ColumnTransformer creates a data preprocessing pipeline that applies\n",
    "# different transformations to different columns\n",
    "preprocessor_X = ColumnTransformer(\n",
    "    # List of transformations to be applied to specific column groups\n",
    "    transformers=[\n",
    "        # 1st Transformer: Numerical columns\n",
    "        ('t_num',\n",
    "         StandardScaler(), # Applies standardization (mean=0, std=1)\n",
    "         X_num_cols),\n",
    "\n",
    "        # 2nd Transformer: Categorical columns\n",
    "        ('t_cat',\n",
    "         # Converts categories to one-hot encoded columns and\n",
    "         #drops first category to avoid multicollinearity\n",
    "         OneHotEncoder(drop='first', sparse_output=False),\n",
    "         X_cat_cols)\n",
    "    ],\n",
    "    # Handling of columns not explicitly transformed\n",
    "    remainder='passthrough' # Keep other columns (if any) - though not applicable here\n",
    ")\n",
    "\n",
    "# This is useful if the model benefits from normalized target values (e.g., linear models).\n",
    "# Often combined with a log transformation (done outside this block in your code: y_train_log).\n",
    "preprocessor_y = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('t_y', StandardScaler(), y_num_col)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Apply transformations using fit_transform on training data and transform on\n",
    "# testing one\n",
    "X_train_processed = preprocessor_X.fit_transform(X_train)\n",
    "X_test_processed = preprocessor_X.transform(X_test)\n",
    "\n",
    "# Converts the log-transformed target (y_train_log) into a DataFrame.\n",
    "# This is done to keep consistency when passing it into preprocessing or model training steps.\n",
    "y_train_log_df = y_train_log.to_frame()\n",
    "\n",
    "print(X_test.shape)        # number of rows in test features\n",
    "print(y_test_log.shape)        # should match X_test\n",
    "print(X_train_processed.shape)\n",
    "print(X_test_processed.shape)\n",
    "print(y_train_log.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23753a4e",
   "metadata": {},
   "source": [
    "## Model Processing\n",
    "\n",
    "The processing encompass the training and execution of several regression models to assess which one is the best performing one.\n",
    "\n",
    "**Models**\n",
    "\n",
    "- **Linear Models**\n",
    "  - Linear Regression\n",
    "  - Ridge\n",
    "  - Lasso\n",
    "- **Tree-Based Models**\n",
    "  - Decision Tree\n",
    "  - Random Forest\n",
    "  - XGBoost\n",
    "- **Instace Based Model**\n",
    "  - KNN\n",
    "- **Kernel-based**\n",
    "  - SVM\n",
    "\n",
    "\n",
    "These models will be executed iteratively to search for the param for each model and also to identify the best performing among all of them using the `RandomizedSearchCV` function.\n",
    "\n",
    "Important to note that the `prices` need to be reverted back from the log transformation to process the price estimation not over a normilized value.\n",
    "\n",
    "### Model Evaluation\n",
    "\n",
    "The function `evaluate_model` runs cross-validation to assess how well a regression model trained on log-transformed prices performs.\n",
    "Thus, it evaluates the model (passed through argument) in two spaces:\n",
    "- Log space - where the model was trained (log(price)) - Uses scikit-learn’s built-in scoring: `neg_mean_squared_error`\n",
    "- Real space - where predictions are converted back to actual prices, making metrics interpretable\n",
    "  - (**Real RMSE**) Uses a custom scorer (`rmse_real_scorer`) that first transforms predictions back with exp() (reverting the log).\n",
    "  - (**Real MAE**) Uses another custom scorer (mae_real_scorer) with inverse log transformation. It makes easier interpret than RMSE because it directly shows average dollar deviation per diamond.\n",
    "- The function `evaluate_model` is designed to assess model performance. Instead of testing the model on a single train/test split, it uses cross-validation (CV) via cross_val_score \n",
    "  - Splits the dataset into cv folds (e.g., 5 folds with KFold(5)). In each fold:\n",
    "    - Trains the model on (cv-1) folds.\n",
    "    - Tests the model on the remaining fold.\n",
    "    - Repeats this process cv times, ensuring every sample is tested once.\n",
    "    - Collects one score per fold, resulting in an array of scores.\n",
    "    - Taking the mean of these scores gives a more reliable estimate of performance than a single train/test split, since it reduces sensitivity to how the data is split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "bd166b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSE in real price space (convert the log prices back to real prices)\n",
    "def rmse_real(y_true_log, y_pred_log):\n",
    "    y_true = np.expm1(y_true_log)   # invert log1p\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "# MAE in real price space (convert the log prices back to real prices)\n",
    "def mae_real(y_true_log, y_pred_log):\n",
    "    y_true = np.expm1(y_true_log)\n",
    "    y_pred = np.expm1(y_pred_log)\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# Custom scorers (rmse_real, mae_real) allow evaluation in real \n",
    "# price space (after inverting the log transformation).\n",
    "rmse_real_scorer = make_scorer(rmse_real, greater_is_better=False)\n",
    "mae_real_scorer = make_scorer(mae_real, greater_is_better=False)\n",
    "\n",
    "def evaluate_model(model, X, y_log, cv):\n",
    "    \"\"\"\n",
    "    Evaluate a regression model trained on log(price).\n",
    "    \n",
    "    Returns a dictionary with:\n",
    "    - log-RMSE\n",
    "    - real-RMSE\n",
    "    - real-MAE\n",
    "    \"\"\"\n",
    "    \n",
    "    # Log RMSE \n",
    "    scores_log = cross_val_score(\n",
    "        model, X, y_log, cv=cv, scoring=\"neg_mean_squared_error\"\n",
    "    )\n",
    "    log_rmse = -scores_log.mean()\n",
    "    \n",
    "    # Real RMSE\n",
    "    scores_real_rmse = cross_val_score(\n",
    "        model, X, y_log, cv=cv, scoring=rmse_real_scorer\n",
    "    )\n",
    "    real_rmse = -scores_real_rmse.mean()\n",
    "    \n",
    "    # Real MAE\n",
    "    scores_real_mae = cross_val_score(\n",
    "        model, X, y_log, cv=cv, scoring=mae_real_scorer\n",
    "    )\n",
    "    real_mae = -scores_real_mae.mean()\n",
    "    \n",
    "    return {\n",
    "        \"Log RMSE\": log_rmse,\n",
    "        \"Real RMSE\": real_rmse,\n",
    "        \"Real MAE\": real_mae\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a46808",
   "metadata": {},
   "source": [
    "### Hyperparameter Optimization\n",
    "\n",
    "The code below is intended to set up a benchmarking and hyperparameter optimization framework for multiple regression models on the diamond prices dataset.\n",
    "\n",
    "It does three main things:\n",
    "- Defines base models to compare.\n",
    "- Sets up hyperparameter search spaces.\n",
    "- Runs cross-validation with RandomizedSearchCV to find the best configuration per model.\n",
    "  - For each model, it uses different sizes of the dataset sampling taking into account the model complexity and time to exexute\n",
    "\n",
    "The SVM model is good on small data and may be impractical on full big datasets. The prediction time also depends on number of support vectors. On the other side, XGBoost is more appropriate for larger datasets.\n",
    "\n",
    "The `RandomizedSearchCV` code block randomly executes samples of `n_iter` combinations from the parameter space. The cross variance can be a integer number or the `KFold` object bringing more precision to the assessment with a cost of more processing power (and time).\n",
    "\n",
    "Thus, the algorithm iterates through all models, call `evaluate_model` for cross-evaluation by computing log-RMSE, real-RMSE, and real-MAE. The inner CV optimization is peroformed by `RandomizedSearchCV` which is responsible for finding the best hyperparameters.\n",
    "\n",
    "Results are stored in two dictionaries:\n",
    "- **results** - summary metrics for each model.\n",
    "- **best_results** - detailed info about best hyperparameters and best fitted estimator.\n",
    "\n",
    "This setup makes it easy to:\n",
    " - Compare baseline and advanced models.\n",
    " - Identify the best-performing model for diamond price prediction.\n",
    " - Document both statistical accuracy and real-world error (USD).\n",
    "\n",
    "The next step is to **retrain** the full dataset on the best model, in this case, the one that presents lower score from the training. This is important to note that SVN and XXBoost execute the trainig (prototyping) over a sample. The sample will retreieved from memory and from the dataset enhancing the proformance execution. In this case, the different samples for SVN and XGBoost (or other model cross variation hyperparaters discovery) are retrived from memory sppeding up the process and making it more consistent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "03156693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing Linear Regression...\n",
      "Using full training data for Linear Regression\n",
      "Best hyperparameters: N/A\n",
      "Time spent: 0.20 seconds\n",
      "\n",
      "Optimizing Ridge...\n",
      "Using full training data for Ridge\n",
      "Best hyperparameters: {'alpha': 0.03906939937054617}\n",
      "Time spent: 6.82 seconds\n",
      "\n",
      "Optimizing Lasso...\n",
      "Using full training data for Lasso\n",
      "Best hyperparameters: {'alpha': 0.03906939937054617}\n",
      "Time spent: 38.20 seconds\n",
      "\n",
      "Optimizing Decision Tree...\n",
      "Using full training data for Decision Tree\n",
      "Best hyperparameters: {'min_samples_split': 5, 'min_samples_leaf': 10, 'max_depth': None}\n",
      "Time spent: 10.16 seconds\n",
      "\n",
      "Optimizing KNN...\n",
      "Using full training data for KNN\n",
      "Best hyperparameters: {'weights': 'distance', 'p': 1, 'n_neighbors': 13}\n",
      "Time spent: 108.71 seconds\n",
      "\n",
      "Optimizing Random Forest...\n",
      "Using full training data for Random Forest\n",
      "Best hyperparameters: {'n_estimators': 500, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': None}\n",
      "Time spent: 1289.26 seconds\n",
      "\n",
      "Optimizing SVM...\n",
      "Using 3% of training data for SVM\n",
      "Best hyperparameters: {'kernel': 'rbf', 'gamma': 0.02069138081114789, 'C': 545.5594781168514}\n",
      "Time spent: 804.17 seconds\n",
      "\n",
      "Optimizing XGBoost...\n",
      "Using full training data for XGBoost\n",
      "Best hyperparameters: {'subsample': 0.8, 'reg_lambda': 1.5, 'reg_alpha': 0, 'n_estimators': 200, 'max_depth': 7, 'learning_rate': 0.2, 'gamma': 0, 'colsample_bytree': 0.8}\n",
      "Time spent: 266.63 seconds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Log RMSE</th>\n",
       "      <th>Real RMSE</th>\n",
       "      <th>Real MAE</th>\n",
       "      <th>Duration (s)</th>\n",
       "      <th>Best Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Linear Regression</th>\n",
       "      <td>0.02</td>\n",
       "      <td>5,686.86</td>\n",
       "      <td>450.34</td>\n",
       "      <td>0.20</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.02</td>\n",
       "      <td>5,742.62</td>\n",
       "      <td>450.82</td>\n",
       "      <td>6.82</td>\n",
       "      <td>0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.08</td>\n",
       "      <td>2,185.92</td>\n",
       "      <td>943.16</td>\n",
       "      <td>38.20</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decision Tree</th>\n",
       "      <td>0.02</td>\n",
       "      <td>768.17</td>\n",
       "      <td>358.52</td>\n",
       "      <td>10.16</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>KNN</th>\n",
       "      <td>0.02</td>\n",
       "      <td>807.58</td>\n",
       "      <td>388.78</td>\n",
       "      <td>108.71</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Random Forest</th>\n",
       "      <td>0.01</td>\n",
       "      <td>705.30</td>\n",
       "      <td>334.06</td>\n",
       "      <td>1,289.26</td>\n",
       "      <td>0.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVM</th>\n",
       "      <td>0.02</td>\n",
       "      <td>976.94</td>\n",
       "      <td>460.50</td>\n",
       "      <td>804.17</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGBoost</th>\n",
       "      <td>0.01</td>\n",
       "      <td>559.03</td>\n",
       "      <td>274.43</td>\n",
       "      <td>266.63</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Log RMSE Real RMSE Real MAE Duration (s) Best Score\n",
       "Linear Regression     0.02  5,686.86   450.34         0.20        NaN\n",
       "Ridge                 0.02  5,742.62   450.82         6.82       0.14\n",
       "Lasso                 0.08  2,185.92   943.16        38.20       0.28\n",
       "Decision Tree         0.02    768.17   358.52        10.16       0.12\n",
       "KNN                   0.02    807.58   388.78       108.71       0.13\n",
       "Random Forest         0.01    705.30   334.06     1,289.26       0.11\n",
       "SVM                   0.02    976.94   460.50       804.17       0.13\n",
       "XGBoost               0.01    559.03   274.43       266.63       0.09"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "base_models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(),\n",
    "    \"KNN\": KNeighborsRegressor(),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=SEED),\n",
    "    \"SVM\": SVR(),\n",
    "    \"XGBoost\": XGBRegressor(random_state=SEED, verbosity=0) # type: ignore\n",
    "}\n",
    "\n",
    "\n",
    "# Define parameter spaces\n",
    "# prepare reasonable search spaces for each model\n",
    "param_spaces = {\n",
    "    \"Linear Regression\": {},  # no hyperparameters to tune\n",
    "    \"Ridge\": {\n",
    "        \"alpha\": np.logspace(-3, 3, 50)\n",
    "    },\n",
    "    \"Lasso\": {\n",
    "        \"alpha\": np.logspace(-3, 3, 50)\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        \"max_depth\": [3, 5, 10, None],\n",
    "        \"min_samples_split\": [2, 5, 10, 20],\n",
    "        \"min_samples_leaf\": [1, 2, 5, 10]\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        \"n_neighbors\": range(2, 50),\n",
    "        \"weights\": [\"uniform\", \"distance\"],\n",
    "        \"p\": [1, 2]  # Manhattan / Euclidean\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"C\": np.logspace(-2, 3, 20),\n",
    "        \"gamma\": np.logspace(-3, 2, 20),\n",
    "        \"kernel\": [\"rbf\", \"poly\", \"sigmoid\"]\n",
    "    },\n",
    "    \"Random Forest\": {\n",
    "        \"n_estimators\": [100, 200, 300, 500],\n",
    "        \"max_depth\": [None, 5, 10, 20],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"max_features\": [\"auto\", \"sqrt\", \"log2\"]\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"n_estimators\": [100, 200, 500],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "        \"max_depth\": [3, 5, 7, 10],\n",
    "        \"subsample\": [0.6, 0.8, 1.0],\n",
    "        \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
    "        \"gamma\": [0, 0.1, 0.2, 0.3],\n",
    "        \"reg_alpha\": [0, 0.01, 0.1, 1],\n",
    "        \"reg_lambda\": [1, 1.5, 2, 5]\n",
    "    }\n",
    "}\n",
    "\n",
    "partitions = 10 \n",
    "kfold = KFold(n_splits=partitions, shuffle=True, random_state=SEED) # makes the partitioning in 10 folds\n",
    "\n",
    "# Prepare models with hyperparameter tuning (Randomized Search)\n",
    "# If no hyperparameters to tune, use the base model directly\n",
    "# KFold CV with 10 splits ensures robust evaluation.\n",
    "searches = {}\n",
    "for name, model in base_models.items():\n",
    "    if param_spaces[name]:  # if we have params to tune\n",
    "        searches[name] = RandomizedSearchCV(\n",
    "            estimator=model,\n",
    "            param_distributions=param_spaces[name],\n",
    "            n_iter=10,   # number of random trials\n",
    "            scoring=\"neg_root_mean_squared_error\",\n",
    "            cv=kfold,        # inner CV for hyperparameter tuning\n",
    "            random_state=SEED,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    else:\n",
    "        searches[name] = model  # LinearRegression (no hyperparams)\n",
    "\n",
    "results = {}\n",
    "best_results = {}\n",
    "times = {}\n",
    "for name, search in searches.items():\n",
    "    print(f\"Optimizing {name}...\")\n",
    "    if name == \"SVM\":  # SVM is too slow for this dataset\n",
    "        print(\"Using 3% of training data for SVM\")\n",
    "        X_train_sample, _, y_train_sample, _ = train_test_split(X_train_processed, \n",
    "                                                                 y_train_log_df, \n",
    "                                                                 train_size=0.03, \n",
    "                                                                 random_state=SEED)\n",
    "    else:\n",
    "        print(\"Using full training data for\", name)\n",
    "        X_train_sample = X_train_processed\n",
    "        y_train_sample = y_train_log_df\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    results[name] = evaluate_model(search, X_train_sample, y_train_sample, cv=3) #kfold)  # outer CV\n",
    "    results[name]['Model'] = search\n",
    "    search.fit(X_train_sample, y_train_sample)  # fit on full training data\n",
    "    print(\"Best hyperparameters:\", search.best_params_ if hasattr(search, 'best_params_' ) else \"N/A\")\n",
    "    end_time = time.time()\n",
    "    results[name]['Duration (s)'] = end_time - start_time\n",
    "    print(f\"Time spent: {(end_time - start_time):.2f} seconds\")\n",
    "    if param_spaces[name]:\n",
    "        best_results[name] = {\n",
    "            \"best_score\": search.best_score_,\n",
    "            \"best_params\": search.best_params_,\n",
    "            \"best_estimator\": search.best_estimator_\n",
    "        }\n",
    "        results[name]['Best Score'] = -search.best_score_\n",
    "    print()\n",
    "\n",
    "pd.set_option(\"display.float_format\", \"{:,.2f}\".format)\n",
    "df_results = pd.DataFrame(results).T\n",
    "df_results.drop(columns=['Model'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c577bad9",
   "metadata": {},
   "source": [
    "### Best Model training and execution\n",
    "\n",
    "The code snippet below selects the best model and correspondinbg optimized parameters to be trained and executed towards the `full` dataset split at the initial stages of this Notebook.\n",
    "\n",
    "The best model is the confronted against the original price values (USD), provided on the dataset, the baseline which corresponds to the simplest model avaialable and the best model.\n",
    "\n",
    "The choice of the baseline refers to the model used to compare all others against. Its role is to establish a minimum performance threshold, if the more robust models cannot beat it, the features, prepropressing and target transformation need to reviewed, however, if the complex models beat it, the complexity (and resource spend) adds value. \n",
    "\n",
    "**Baseline**: `Linear Regression` (with same preprocessing and transformation applied to all models)\n",
    "`Linear Regression` has less than 1 second execution time towards a full dataset and Real MAE and MRSE don't deviate much from the best result found on the XGBoost. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "dd118b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Model Test RMSE: 3,569,600.19\n",
      "Baseline Model Test MAE: 34,798.83\n",
      "Baseline Model Test R²: -800556.726\n",
      "Regressor: LinearRegression()\n",
      "Score: 0.9679\n",
      "\n",
      "Best Model Test RMSE: 568.14\n",
      "Best Model Test MAE: 280.38\n",
      "Best Model Test R²: 0.980\n",
      "Regressor: XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
      "             colsample_bylevel=None, colsample_bynode=None,\n",
      "             colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
      "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "             gamma=0, grow_policy=None, importance_type=None,\n",
      "             interaction_constraints=None, learning_rate=0.2, max_bin=None,\n",
      "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "             max_delta_step=None, max_depth=7, max_leaves=None,\n",
      "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "             multi_strategy=None, n_estimators=200, n_jobs=None,\n",
      "             num_parallel_tree=None, random_state=42, ...)\n",
      "Score: 0.9916\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train and predict with the given model\n",
    "def predict_model(model, X_train, y_train_log, X_test):\n",
    "    baseline_model.fit(X_train, y_train_log)\n",
    "    y_pred_log = model.predict(X_test)\n",
    "    y_pred_real = np.expm1(y_pred_log)\n",
    "    return y_pred_real\n",
    "\n",
    "# Print the results\n",
    "def print_results(model_name, model, y_test_real, y_pred_real):\n",
    "    rmse = root_mean_squared_error(y_test_real, y_pred_real)\n",
    "    r2 = r2_score(y_test_real, y_pred_real)\n",
    "    mae = mean_absolute_error(y_test_real, y_pred_real)\n",
    "\n",
    "    print(f\"{model_name} Test RMSE: {rmse:,.2f}\")\n",
    "    print(f\"{model_name} Test MAE: {mae:,.2f}\")\n",
    "    print(f\"{model_name} Test R²: {r2:.3f}\")\n",
    "    print(f\"Regressor: {model}\")\n",
    "\n",
    "# Fit the best model on the *entire training set*\n",
    "# select the best model by its score\n",
    "best_model_name = max(best_results, key=lambda k: best_results[k][\"best_score\"])\n",
    "best_model = best_results[best_model_name][\"best_estimator\"]\n",
    "\n",
    "baseline_model = results[\"Linear Regression\"][\"Model\"]\n",
    "\n",
    "y_baseline_pred_real = predict_model(baseline_model, X_train_processed, y_train_log_df, X_test_processed)\n",
    "y_pred_real = predict_model(best_model, X_train_processed, y_train_log_df, X_test_processed)\n",
    "\n",
    "y_test_real = np.expm1(y_test_log)\n",
    "\n",
    "print_results(\"Baseline Model\", baseline_model, y_test_real, y_baseline_pred_real)\n",
    "print(f\"Score: {baseline_model.score(X_test_processed, y_test_log):,.4f}\")\n",
    "print()\n",
    "\n",
    "print_results(\"Best Model\", best_model, y_test_real, y_pred_real)\n",
    "print(f\"Score: {best_model.score(X_test_processed, y_test_log):,.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f84070",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "The whole process follows the following flow:\n",
    "1. Fectch dataset\n",
    "2. split in train/test\n",
    "3. preprocess/pipeline building\n",
    "4. Model Selection/Tunning\n",
    "5. Best Model / Base Line \n",
    "6. Final Model Training\n",
    "\n",
    "Baseline: Linear regression models behave similarly, in which `Linear Regression` is a solid benchmark but tree-based (Random Forest, XGBoost) significantly improve prediction accuracy (with not much resource addition)\n",
    "\n",
    "XGBoost has the most efficient results and competitive accuracy. It turns out to be the most indicated model (from the ones tested in this exercise) to run over the Diamonds Prices dataset containing over 50K registers.\n",
    "\n",
    "SVN is the least efficient. It presents high accuracy on sample but computationally expensive at scale. It took too long to execute which required a very restricted sample representing less than 5% of the entire dataset. \n",
    "\n",
    "Baseline against Advanced Models: Linear Regression is a solid benchmark, but tree-based ensembles (Random Forest, XGBoost) significantly improve prediction accuracy.\n",
    "\n",
    "SVM vs. XGBoost: SVM performed surprisingly well on smaller samples, but scalability is a concern. Therefore, XGBoost is more suitable for full dataset deployment due to its balance of performance and execution time.\n",
    "\n",
    "**Execution Strategy**: Using representative samples for hyperparameter tuning (fast) followed by retraining on the full dataset (final model) ensures both time efficiency and model robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "6ab280be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_b09cf\" style='display:inline;'>\n",
       "  <caption>Original</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b09cf_level0_col0\" class=\"col_heading level0 col0\" >price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b09cf_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_b09cf_row0_col0\" class=\"data row0 col0\" >335.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09cf_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_b09cf_row1_col0\" class=\"data row1 col0\" >336.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09cf_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_b09cf_row2_col0\" class=\"data row2 col0\" >337.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09cf_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_b09cf_row3_col0\" class=\"data row3 col0\" >358.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09cf_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_b09cf_row4_col0\" class=\"data row4 col0\" >360.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09cf_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_b09cf_row5_col0\" class=\"data row5 col0\" >363.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09cf_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_b09cf_row6_col0\" class=\"data row6 col0\" >364.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09cf_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_b09cf_row7_col0\" class=\"data row7 col0\" >366.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09cf_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_b09cf_row8_col0\" class=\"data row8 col0\" >367.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09cf_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_b09cf_row9_col0\" class=\"data row9 col0\" >367.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09cf_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_b09cf_row10_col0\" class=\"data row10 col0\" >367.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09cf_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_b09cf_row11_col0\" class=\"data row11 col0\" >374.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09cf_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_b09cf_row12_col0\" class=\"data row12 col0\" >375.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09cf_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_b09cf_row13_col0\" class=\"data row13 col0\" >378.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09cf_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_b09cf_row14_col0\" class=\"data row14 col0\" >378.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09cf_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_b09cf_row15_col0\" class=\"data row15 col0\" >380.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09cf_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_b09cf_row16_col0\" class=\"data row16 col0\" >382.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09cf_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_b09cf_row17_col0\" class=\"data row17 col0\" >383.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09cf_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_b09cf_row18_col0\" class=\"data row18 col0\" >384.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b09cf_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_b09cf_row19_col0\" class=\"data row19 col0\" >384.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_ecc2e\" style='display:inline; margin-left: 20px;'>\n",
       "  <caption>Baseline</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ecc2e_level0_col0\" class=\"col_heading level0 col0\" >price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ecc2e_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_ecc2e_row0_col0\" class=\"data row0 col0\" >279.030288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ecc2e_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_ecc2e_row1_col0\" class=\"data row1 col0\" >313.264299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ecc2e_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_ecc2e_row2_col0\" class=\"data row2 col0\" >315.489590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ecc2e_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_ecc2e_row3_col0\" class=\"data row3 col0\" >316.607324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ecc2e_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_ecc2e_row4_col0\" class=\"data row4 col0\" >318.690171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ecc2e_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_ecc2e_row5_col0\" class=\"data row5 col0\" >319.486916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ecc2e_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_ecc2e_row6_col0\" class=\"data row6 col0\" >321.610862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ecc2e_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_ecc2e_row7_col0\" class=\"data row7 col0\" >323.493550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ecc2e_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_ecc2e_row8_col0\" class=\"data row8 col0\" >326.981917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ecc2e_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_ecc2e_row9_col0\" class=\"data row9 col0\" >327.478050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ecc2e_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_ecc2e_row10_col0\" class=\"data row10 col0\" >331.307922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ecc2e_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_ecc2e_row11_col0\" class=\"data row11 col0\" >335.515016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ecc2e_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_ecc2e_row12_col0\" class=\"data row12 col0\" >346.780677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ecc2e_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_ecc2e_row13_col0\" class=\"data row13 col0\" >346.929155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ecc2e_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_ecc2e_row14_col0\" class=\"data row14 col0\" >352.289966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ecc2e_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_ecc2e_row15_col0\" class=\"data row15 col0\" >356.300925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ecc2e_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_ecc2e_row16_col0\" class=\"data row16 col0\" >360.196365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ecc2e_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_ecc2e_row17_col0\" class=\"data row17 col0\" >360.514248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ecc2e_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_ecc2e_row18_col0\" class=\"data row18 col0\" >367.044964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ecc2e_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_ecc2e_row19_col0\" class=\"data row19 col0\" >369.761352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e2cb6\" style='display:inline; margin-right: 20px;'>\n",
       "  <caption>Best Preditor</caption>\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e2cb6_level0_col0\" class=\"col_heading level0 col0\" >price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e2cb6_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_e2cb6_row0_col0\" class=\"data row0 col0\" >338.165436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e2cb6_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_e2cb6_row1_col0\" class=\"data row1 col0\" >346.479980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e2cb6_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_e2cb6_row2_col0\" class=\"data row2 col0\" >351.645660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e2cb6_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_e2cb6_row3_col0\" class=\"data row3 col0\" >363.463531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e2cb6_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_e2cb6_row4_col0\" class=\"data row4 col0\" >365.821136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e2cb6_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_e2cb6_row5_col0\" class=\"data row5 col0\" >365.913666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e2cb6_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_e2cb6_row6_col0\" class=\"data row6 col0\" >368.204346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e2cb6_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_e2cb6_row7_col0\" class=\"data row7 col0\" >373.891693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e2cb6_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_e2cb6_row8_col0\" class=\"data row8 col0\" >379.278595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e2cb6_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_e2cb6_row9_col0\" class=\"data row9 col0\" >380.431793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e2cb6_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_e2cb6_row10_col0\" class=\"data row10 col0\" >382.578552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e2cb6_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_e2cb6_row11_col0\" class=\"data row11 col0\" >382.733673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e2cb6_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_e2cb6_row12_col0\" class=\"data row12 col0\" >386.542877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e2cb6_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_e2cb6_row13_col0\" class=\"data row13 col0\" >388.305542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e2cb6_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_e2cb6_row14_col0\" class=\"data row14 col0\" >389.929718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e2cb6_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_e2cb6_row15_col0\" class=\"data row15 col0\" >391.149231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e2cb6_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_e2cb6_row16_col0\" class=\"data row16 col0\" >392.034332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e2cb6_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_e2cb6_row17_col0\" class=\"data row17 col0\" >392.187683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e2cb6_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_e2cb6_row18_col0\" class=\"data row18 col0\" >392.262848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e2cb6_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_e2cb6_row19_col0\" class=\"data row19 col0\" >392.694031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def to_df(y):\n",
    "    df = pd.DataFrame(y, columns=[\"price\"])\n",
    "    return df.sort_values(by='price').head(20).reset_index(drop=True)\n",
    "\n",
    "df_best_model = to_df(y_pred_real)\n",
    "df_baseline_model = to_df(y_baseline_pred_real)\n",
    "y_test_real = y_test_real.sort_values(by='price').head(20).reset_index(drop=True)\n",
    "\n",
    "df_predictor_styler = df_best_model.style.set_table_attributes(\"style='display:inline; margin-right: 20px;'\").set_caption('Best Preditor')\n",
    "df_original_styler = y_test_real.style.set_table_attributes(\"style='display:inline;'\").set_caption('Original')\n",
    "df_baseline_styler = df_baseline_model.style.set_table_attributes(\"style='display:inline; margin-left: 20px;'\").set_caption('Baseline')\n",
    "\n",
    "# Display side by side the first 20 predicted prices from best model, \n",
    "# baseline model and original prices\n",
    "display_html(df_original_styler._repr_html_() + \n",
    "             df_baseline_styler._repr_html_() + \n",
    "             df_predictor_styler._repr_html_(), \n",
    "             raw=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
